"""
Alert management service for the Infrastructure Monitoring Agent.

This module provides alert generation, deduplication, and tracking functionality
for SLA violations and persistent downtime monitoring across the vessel fleet.
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass
from enum import Enum

from ..models.data_models import VesselMetrics, ComponentStatus, SLAStatus
from ..models.enums import ComponentType, OperationalStatus, IssueSeverity, AlertType, AlertSeverity
from .database import DatabaseService


logger = logging.getLogger(__name__)


@dataclass
class Alert:
    """Represents an alert generated by the monitoring system."""
    
    id: Optional[int]
    vessel_id: str
    component_type: ComponentType
    alert_type: AlertType
    severity: AlertSeverity
    message: str
    metadata: Dict[str, Any]
    created_at: datetime
    is_resolved: bool = False
    resolved_at: Optional[datetime] = None
    
    def get_unique_key(self) -> str:
        """Generate a unique key for alert deduplication."""
        return f"{self.vessel_id}:{self.component_type.value}:{self.alert_type.value}"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert alert to dictionary for serialization."""
        return {
            'id': self.id,
            'vessel_id': self.vessel_id,
            'component_type': self.component_type.value,
            'alert_type': self.alert_type.value,
            'severity': self.severity.value,
            'message': self.message,
            'metadata': self.metadata,
            'created_at': self.created_at.isoformat(),
            'is_resolved': self.is_resolved,
            'resolved_at': self.resolved_at.isoformat() if self.resolved_at else None
        }


class AlertManager:
    """
    Service for managing alerts across the vessel fleet.
    
    This class handles alert generation, deduplication, persistence, and tracking
    for SLA violations and persistent downtime monitoring.
    """
    
    def __init__(self, database_service: DatabaseService, sla_threshold: float = 95.0):
        """
        Initialize the alert manager.
        
        Args:
            database_service: Database service for persistent storage
            sla_threshold: SLA threshold percentage (default 95%)
        """
        self.database = database_service
        self.sla_threshold = sla_threshold
        self.persistent_downtime_threshold = timedelta(days=3)
        
        # In-memory tracking for active alerts to prevent duplicates
        self._active_alerts: Dict[str, Alert] = {}
        self._load_active_alerts()
        
        logger.info(
            f"Initialized AlertManager with SLA threshold: {sla_threshold}%, "
            f"persistent downtime threshold: {self.persistent_downtime_threshold}"
        )
    
    def _load_active_alerts(self) -> None:
        """Load active alerts from database into memory for deduplication."""
        try:
            # This would typically query the database for unresolved alerts
            # For now, we'll start with an empty set and build it as alerts are generated
            self._active_alerts.clear()
            logger.debug("Loaded active alerts from database")
        except Exception as e:
            logger.error(f"Failed to load active alerts: {e}")
            self._active_alerts.clear()
    
    def process_vessel_metrics(self, vessel_metrics: VesselMetrics) -> List[Alert]:
        """
        Process vessel metrics and generate appropriate alerts.
        
        Args:
            vessel_metrics: Complete metrics for a vessel
            
        Returns:
            List of alerts generated for this vessel
        """
        alerts = []
        
        # Check each component for SLA violations and persistent downtime
        for component_type, component_status in vessel_metrics.get_all_components().items():
            # Check for SLA violations
            sla_alerts = self._check_sla_violation(
                vessel_metrics.vessel_id, 
                component_type, 
                component_status
            )
            alerts.extend(sla_alerts)
            
            # Check for persistent downtime
            downtime_alerts = self._check_persistent_downtime(
                vessel_metrics.vessel_id,
                component_type,
                component_status
            )
            alerts.extend(downtime_alerts)
            
            # Check for component recovery
            recovery_alerts = self._check_component_recovery(
                vessel_metrics.vessel_id,
                component_type,
                component_status
            )
            alerts.extend(recovery_alerts)
        
        logger.info(
            f"Processed metrics for vessel {vessel_metrics.vessel_id}, "
            f"generated {len(alerts)} alerts"
        )
        
        return alerts
    
    def _check_sla_violation(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> List[Alert]:
        """
        Check for SLA violations and generate alerts.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            
        Returns:
            List of SLA violation alerts
        """
        alerts = []
        
        # Check if uptime is below SLA threshold
        if component_status.uptime_percentage < self.sla_threshold:
            alert_key = f"{vessel_id}:{component_type.value}:{AlertType.SLA_VIOLATION.value}"
            
            # Check if we already have an active alert for this violation
            if alert_key not in self._active_alerts:
                severity = self._determine_sla_violation_severity(component_status.uptime_percentage)
                
                alert = Alert(
                    id=None,
                    vessel_id=vessel_id,
                    component_type=component_type,
                    alert_type=AlertType.SLA_VIOLATION,
                    severity=severity,
                    message=self._generate_sla_violation_message(
                        vessel_id, component_type, component_status
                    ),
                    metadata={
                        'uptime_percentage': component_status.uptime_percentage,
                        'sla_threshold': self.sla_threshold,
                        'current_status': component_status.current_status.value,
                        'downtime_aging_hours': component_status.downtime_aging.total_seconds() / 3600,
                        'last_ping_time': component_status.last_ping_time.isoformat()
                    },
                    created_at=datetime.utcnow()
                )
                
                # Persist the alert
                alert.id = self._persist_alert(alert)
                
                # Add to active alerts for deduplication
                self._active_alerts[alert_key] = alert
                alerts.append(alert)
                
                logger.warning(
                    f"Generated SLA violation alert for {component_type.value} "
                    f"on vessel {vessel_id}: {component_status.uptime_percentage:.2f}% uptime"
                )
        
        return alerts
    
    def _check_persistent_downtime(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> List[Alert]:
        """
        Check for persistent downtime and generate alerts.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            
        Returns:
            List of persistent downtime alerts
        """
        alerts = []
        
        # Check if component is down and downtime exceeds threshold
        if (component_status.current_status == OperationalStatus.DOWN and
            component_status.downtime_aging >= self.persistent_downtime_threshold):
            
            alert_key = f"{vessel_id}:{component_type.value}:{AlertType.PERSISTENT_DOWNTIME.value}"
            
            # Check if we already have an active alert for this persistent downtime
            if alert_key not in self._active_alerts:
                alert = Alert(
                    id=None,
                    vessel_id=vessel_id,
                    component_type=component_type,
                    alert_type=AlertType.PERSISTENT_DOWNTIME,
                    severity=AlertSeverity.CRITICAL,
                    message=self._generate_persistent_downtime_message(
                        vessel_id, component_type, component_status
                    ),
                    metadata={
                        'downtime_aging_hours': component_status.downtime_aging.total_seconds() / 3600,
                        'downtime_threshold_hours': self.persistent_downtime_threshold.total_seconds() / 3600,
                        'uptime_percentage': component_status.uptime_percentage,
                        'last_ping_time': component_status.last_ping_time.isoformat(),
                        'requires_ticket_creation': True
                    },
                    created_at=datetime.utcnow()
                )
                
                # Persist the alert
                alert.id = self._persist_alert(alert)
                
                # Add to active alerts for deduplication
                self._active_alerts[alert_key] = alert
                alerts.append(alert)
                
                logger.critical(
                    f"Generated persistent downtime alert for {component_type.value} "
                    f"on vessel {vessel_id}: down for "
                    f"{component_status.downtime_aging.total_seconds() / 3600:.1f} hours"
                )
        
        return alerts
    
    def _check_component_recovery(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> List[Alert]:
        """
        Check for component recovery and resolve existing alerts.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            
        Returns:
            List of recovery alerts
        """
        alerts = []
        
        # Check if component is now up and we have active alerts for it
        if component_status.current_status == OperationalStatus.UP:
            # Check for active SLA violation alerts
            sla_alert_key = f"{vessel_id}:{component_type.value}:{AlertType.SLA_VIOLATION.value}"
            downtime_alert_key = f"{vessel_id}:{component_type.value}:{AlertType.PERSISTENT_DOWNTIME.value}"
            
            resolved_alerts = []
            
            # Resolve SLA violation alert if exists and uptime is back above threshold
            if (sla_alert_key in self._active_alerts and 
                component_status.uptime_percentage >= self.sla_threshold):
                
                resolved_alerts.append(self._active_alerts[sla_alert_key])
                del self._active_alerts[sla_alert_key]
            
            # Resolve persistent downtime alert if exists
            if downtime_alert_key in self._active_alerts:
                resolved_alerts.append(self._active_alerts[downtime_alert_key])
                del self._active_alerts[downtime_alert_key]
            
            # Generate recovery alert if we resolved any alerts
            if resolved_alerts:
                recovery_alert = Alert(
                    id=None,
                    vessel_id=vessel_id,
                    component_type=component_type,
                    alert_type=AlertType.COMPONENT_RECOVERY,
                    severity=AlertSeverity.INFO,
                    message=self._generate_recovery_message(
                        vessel_id, component_type, component_status
                    ),
                    metadata={
                        'uptime_percentage': component_status.uptime_percentage,
                        'resolved_alert_count': len(resolved_alerts),
                        'resolved_alert_types': [alert.alert_type.value for alert in resolved_alerts],
                        'recovery_time': datetime.utcnow().isoformat()
                    },
                    created_at=datetime.utcnow()
                )
                
                # Persist the recovery alert
                recovery_alert.id = self._persist_alert(recovery_alert)
                alerts.append(recovery_alert)
                
                # Mark resolved alerts as resolved in database
                for resolved_alert in resolved_alerts:
                    if resolved_alert.id:
                        self._resolve_alert(resolved_alert.id)
                
                logger.info(
                    f"Generated recovery alert for {component_type.value} "
                    f"on vessel {vessel_id}, resolved {len(resolved_alerts)} alerts"
                )
        
        return alerts
    
    def _determine_sla_violation_severity(self, uptime_percentage: float) -> AlertSeverity:
        """
        Determine the severity of an SLA violation based on uptime percentage.
        
        Args:
            uptime_percentage: Current uptime percentage
            
        Returns:
            Alert severity level
        """
        if uptime_percentage < 50:
            return AlertSeverity.CRITICAL
        elif uptime_percentage < 80:
            return AlertSeverity.WARNING
        else:
            return AlertSeverity.WARNING  # Below 95% but above 80%
    
    def _generate_sla_violation_message(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> str:
        """Generate a descriptive message for SLA violation alerts."""
        return (
            f"SLA violation detected for {component_type.value.title()} on vessel {vessel_id}. "
            f"Uptime: {component_status.uptime_percentage:.2f}% "
            f"(threshold: {self.sla_threshold}%). "
            f"Current status: {component_status.current_status.value.upper()}. "
            f"Downtime aging: {component_status.downtime_aging.total_seconds() / 3600:.1f} hours."
        )
    
    def _generate_persistent_downtime_message(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> str:
        """Generate a descriptive message for persistent downtime alerts."""
        downtime_hours = component_status.downtime_aging.total_seconds() / 3600
        threshold_hours = self.persistent_downtime_threshold.total_seconds() / 3600
        
        return (
            f"Persistent downtime detected for {component_type.value.title()} on vessel {vessel_id}. "
            f"Component has been down for {downtime_hours:.1f} hours "
            f"(threshold: {threshold_hours:.1f} hours). "
            f"JIRA ticket creation required."
        )
    
    def _generate_recovery_message(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> str:
        """Generate a descriptive message for component recovery alerts."""
        return (
            f"Component recovery detected for {component_type.value.title()} on vessel {vessel_id}. "
            f"Current uptime: {component_status.uptime_percentage:.2f}%. "
            f"Status: {component_status.current_status.value.upper()}. "
            f"Previous alerts have been resolved."
        )
    
    def _persist_alert(self, alert: Alert) -> int:
        """
        Persist an alert to the database.
        
        Args:
            alert: Alert to persist
            
        Returns:
            ID of the persisted alert
        """
        try:
            alert_id = self.database.record_alert(
                vessel_id=alert.vessel_id,
                component_type=alert.component_type,
                alert_type=alert.alert_type.value,
                severity=alert.severity.value,
                message=alert.message,
                metadata=alert.metadata
            )
            return alert_id
        except Exception as e:
            logger.error(f"Failed to persist alert: {e}")
            raise
    
    def _resolve_alert(self, alert_id: int) -> None:
        """
        Mark an alert as resolved in the database.
        
        Args:
            alert_id: ID of the alert to resolve
        """
        try:
            self.database.resolve_alert(alert_id)
        except Exception as e:
            logger.error(f"Failed to resolve alert {alert_id}: {e}")
            raise
    
    def get_active_alerts(
        self,
        vessel_id: Optional[str] = None,
        component_type: Optional[ComponentType] = None,
        alert_type: Optional[AlertType] = None
    ) -> List[Alert]:
        """
        Get currently active alerts with optional filtering.
        
        Args:
            vessel_id: Optional filter by vessel ID
            component_type: Optional filter by component type
            alert_type: Optional filter by alert type
            
        Returns:
            List of active alerts matching the filters
        """
        filtered_alerts = []
        
        for alert in self._active_alerts.values():
            # Apply filters
            if vessel_id and alert.vessel_id != vessel_id:
                continue
            if component_type and alert.component_type != component_type:
                continue
            if alert_type and alert.alert_type != alert_type:
                continue
            
            filtered_alerts.append(alert)
        
        # Sort by creation time (newest first)
        filtered_alerts.sort(key=lambda a: a.created_at, reverse=True)
        
        return filtered_alerts
    
    def get_alerts_requiring_tickets(self) -> List[Alert]:
        """
        Get alerts that require JIRA ticket creation.
        
        Returns:
            List of persistent downtime alerts requiring tickets
        """
        ticket_alerts = []
        
        for alert in self._active_alerts.values():
            if (alert.alert_type == AlertType.PERSISTENT_DOWNTIME and
                alert.metadata.get('requires_ticket_creation', False)):
                ticket_alerts.append(alert)
        
        return ticket_alerts
    
    def mark_ticket_created(self, alert_id: int, ticket_id: str) -> None:
        """
        Mark that a JIRA ticket has been created for an alert.
        
        Args:
            alert_id: ID of the alert
            ticket_id: ID of the created JIRA ticket
        """
        # Find the alert in active alerts
        for alert in self._active_alerts.values():
            if alert.id == alert_id:
                alert.metadata['ticket_created'] = True
                alert.metadata['ticket_id'] = ticket_id
                alert.metadata['ticket_created_at'] = datetime.utcnow().isoformat()
                alert.metadata['requires_ticket_creation'] = False
                
                logger.info(
                    f"Marked alert {alert_id} as having ticket {ticket_id} created"
                )
                break
    
    def get_alert_statistics(self, days_back: int = 30) -> Dict[str, Any]:
        """
        Get statistics about alerts over a specified period.
        
        Args:
            days_back: Number of days to analyze
            
        Returns:
            Dictionary containing alert statistics
        """
        # This would typically query the database for historical alert data
        # For now, return basic statistics from active alerts
        
        total_active = len(self._active_alerts)
        sla_violations = len([a for a in self._active_alerts.values() 
                             if a.alert_type == AlertType.SLA_VIOLATION])
        persistent_downtime = len([a for a in self._active_alerts.values() 
                                  if a.alert_type == AlertType.PERSISTENT_DOWNTIME])
        
        # Group by vessel
        vessels_with_alerts = set(alert.vessel_id for alert in self._active_alerts.values())
        
        # Group by component type
        component_alerts = {}
        for alert in self._active_alerts.values():
            comp_type = alert.component_type.value
            component_alerts[comp_type] = component_alerts.get(comp_type, 0) + 1
        
        return {
            'total_active_alerts': total_active,
            'sla_violations': sla_violations,
            'persistent_downtime_alerts': persistent_downtime,
            'vessels_with_alerts': len(vessels_with_alerts),
            'alerts_by_component': component_alerts,
            'analysis_period_days': days_back
        }
    
    def monitor_persistent_downtime(self, vessel_metrics_list: List[VesselMetrics]) -> List[Alert]:
        """
        Monitor all vessels for persistent downtime and trigger ticket creation process.
        
        This method specifically focuses on detecting components that have been down
        for more than the threshold period and require JIRA ticket creation.
        
        Args:
            vessel_metrics_list: List of metrics for all vessels
            
        Returns:
            List of persistent downtime alerts requiring ticket creation
        """
        persistent_downtime_alerts = []
        
        for vessel_metrics in vessel_metrics_list:
            for component_type, component_status in vessel_metrics.get_all_components().items():
                # Check if component meets persistent downtime criteria
                if self._requires_ticket_creation(vessel_metrics.vessel_id, component_type, component_status):
                    # Generate persistent downtime alert if not already active
                    alert_key = f"{vessel_metrics.vessel_id}:{component_type.value}:{AlertType.PERSISTENT_DOWNTIME.value}"
                    
                    if alert_key not in self._active_alerts:
                        alert = self._create_persistent_downtime_alert(
                            vessel_metrics.vessel_id,
                            component_type,
                            component_status
                        )
                        persistent_downtime_alerts.append(alert)
                        
                        # Log comprehensive audit trail
                        self._log_persistent_downtime_detection(
                            vessel_metrics.vessel_id,
                            component_type,
                            component_status,
                            alert
                        )
        
        logger.info(
            f"Monitored {len(vessel_metrics_list)} vessels for persistent downtime, "
            f"found {len(persistent_downtime_alerts)} components requiring tickets"
        )
        
        return persistent_downtime_alerts
    
    def _requires_ticket_creation(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> bool:
        """
        Determine if a component requires JIRA ticket creation.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            
        Returns:
            True if ticket creation is required
        """
        # Component must be down
        if component_status.current_status != OperationalStatus.DOWN:
            return False
        
        # Downtime must exceed threshold
        if component_status.downtime_aging < self.persistent_downtime_threshold:
            return False
        
        # Check if we already have an active persistent downtime alert
        alert_key = f"{vessel_id}:{component_type.value}:{AlertType.PERSISTENT_DOWNTIME.value}"
        if alert_key in self._active_alerts:
            # Check if ticket has already been created for this alert
            existing_alert = self._active_alerts[alert_key]
            if existing_alert.metadata.get('ticket_created', False):
                return False
        
        return True
    
    def _create_persistent_downtime_alert(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> Alert:
        """
        Create a persistent downtime alert with comprehensive metadata.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            
        Returns:
            Created persistent downtime alert
        """
        downtime_hours = component_status.downtime_aging.total_seconds() / 3600
        threshold_hours = self.persistent_downtime_threshold.total_seconds() / 3600
        
        # Generate historical context for the alert
        historical_context = self._generate_historical_context(
            vessel_id, component_type, component_status
        )
        
        alert = Alert(
            id=None,
            vessel_id=vessel_id,
            component_type=component_type,
            alert_type=AlertType.PERSISTENT_DOWNTIME,
            severity=AlertSeverity.CRITICAL,
            message=self._generate_persistent_downtime_message(
                vessel_id, component_type, component_status
            ),
            metadata={
                'downtime_aging_hours': downtime_hours,
                'downtime_threshold_hours': threshold_hours,
                'uptime_percentage': component_status.uptime_percentage,
                'last_ping_time': component_status.last_ping_time.isoformat(),
                'requires_ticket_creation': True,
                'ticket_created': False,
                'historical_context': historical_context,
                'detection_time': datetime.utcnow().isoformat(),
                'severity_justification': f"Component down for {downtime_hours:.1f}h exceeds {threshold_hours:.1f}h threshold"
            },
            created_at=datetime.utcnow()
        )
        
        # Persist the alert
        alert.id = self._persist_alert(alert)
        
        # Add to active alerts for deduplication
        alert_key = alert.get_unique_key()
        self._active_alerts[alert_key] = alert
        
        return alert
    
    def _generate_historical_context(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus
    ) -> str:
        """
        Generate historical context for persistent downtime alerts.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            
        Returns:
            Historical context string
        """
        try:
            # Get recent violation history for this component
            violations = self.database.get_violation_history(
                vessel_id=vessel_id,
                component_type=component_type,
                days_back=30
            )
            
            # Get component status trends
            trends = self.database.get_component_status_trends(
                vessel_id=vessel_id,
                component_type=component_type,
                days_back=7
            )
            
            context_parts = []
            
            # Add current status information
            context_parts.append(
                f"Current Status: {component_status.current_status.value.upper()}"
            )
            context_parts.append(
                f"Uptime Percentage: {component_status.uptime_percentage:.2f}%"
            )
            context_parts.append(
                f"Downtime Duration: {component_status.downtime_aging.total_seconds() / 3600:.1f} hours"
            )
            context_parts.append(
                f"Last Ping: {component_status.last_ping_time.strftime('%Y-%m-%d %H:%M:%S UTC')}"
            )
            
            # Add violation history if available
            if violations:
                context_parts.append(f"\nRecent SLA Violations (30 days): {len(violations)}")
                resolved_violations = [v for v in violations if v.get('is_resolved', False)]
                context_parts.append(f"Resolved Violations: {len(resolved_violations)}")
                
                if violations:
                    latest_violation = violations[0]  # Most recent
                    context_parts.append(
                        f"Latest Violation: {latest_violation['violation_start']} "
                        f"({latest_violation['uptime_percentage']:.2f}% uptime)"
                    )
            
            # Add trend information if available
            if trends:
                context_parts.append(f"\nStatus Trend (7 days): {len(trends)} data points")
                down_periods = [t for t in trends if t['current_status'] == OperationalStatus.DOWN]
                context_parts.append(f"Down Status Occurrences: {len(down_periods)}")
            
            return "\n".join(context_parts)
            
        except Exception as e:
            logger.warning(f"Failed to generate historical context: {e}")
            return (
                f"Current Status: {component_status.current_status.value.upper()}\n"
                f"Uptime: {component_status.uptime_percentage:.2f}%\n"
                f"Downtime: {component_status.downtime_aging.total_seconds() / 3600:.1f} hours\n"
                f"Historical data unavailable due to error."
            )
    
    def _log_persistent_downtime_detection(
        self,
        vessel_id: str,
        component_type: ComponentType,
        component_status: ComponentStatus,
        alert: Alert
    ) -> None:
        """
        Log comprehensive audit trail for persistent downtime detection.
        
        Args:
            vessel_id: ID of the vessel
            component_type: Type of component
            component_status: Current component status
            alert: Generated alert
        """
        audit_data = {
            'event': 'persistent_downtime_detected',
            'vessel_id': vessel_id,
            'component_type': component_type.value,
            'alert_id': alert.id,
            'downtime_hours': component_status.downtime_aging.total_seconds() / 3600,
            'threshold_hours': self.persistent_downtime_threshold.total_seconds() / 3600,
            'uptime_percentage': component_status.uptime_percentage,
            'detection_time': alert.created_at.isoformat(),
            'requires_ticket': True
        }
        
        logger.critical(
            f"PERSISTENT DOWNTIME DETECTED - Vessel: {vessel_id}, "
            f"Component: {component_type.value}, "
            f"Duration: {audit_data['downtime_hours']:.1f}h, "
            f"Alert ID: {alert.id}"
        )
        
        # Additional structured logging for audit purposes
        logger.info(f"Audit Trail: {audit_data}")
    
    def maintain_alert_status(self, vessel_metrics_list: List[VesselMetrics]) -> Dict[str, int]:
        """
        Maintain alert status by checking if issues have been resolved.
        
        This method updates alert status based on current component states
        and maintains the alert lifecycle until issues are resolved.
        
        Args:
            vessel_metrics_list: List of current metrics for all vessels
            
        Returns:
            Dictionary with counts of maintained, resolved, and new alerts
        """
        maintained_count = 0
        resolved_count = 0
        new_count = 0
        
        # Track which alerts we've seen in current metrics
        current_alert_keys = set()
        
        # Process current metrics and update alert status
        for vessel_metrics in vessel_metrics_list:
            for component_type, component_status in vessel_metrics.get_all_components().items():
                # Generate alert key for this component
                sla_alert_key = f"{vessel_metrics.vessel_id}:{component_type.value}:{AlertType.SLA_VIOLATION.value}"
                downtime_alert_key = f"{vessel_metrics.vessel_id}:{component_type.value}:{AlertType.PERSISTENT_DOWNTIME.value}"
                
                # Check SLA violation status
                if component_status.uptime_percentage < self.sla_threshold:
                    current_alert_keys.add(sla_alert_key)
                    if sla_alert_key in self._active_alerts:
                        maintained_count += 1
                    else:
                        # This would be handled by process_vessel_metrics
                        new_count += 1
                
                # Check persistent downtime status
                if (component_status.current_status == OperationalStatus.DOWN and
                    component_status.downtime_aging >= self.persistent_downtime_threshold):
                    current_alert_keys.add(downtime_alert_key)
                    if downtime_alert_key in self._active_alerts:
                        maintained_count += 1
                    else:
                        # This would be handled by process_vessel_metrics
                        new_count += 1
        
        # Identify alerts that should be resolved (no longer present in current state)
        alerts_to_resolve = []
        for alert_key, alert in list(self._active_alerts.items()):
            if alert_key not in current_alert_keys:
                # Alert condition no longer exists, mark for resolution
                alerts_to_resolve.append((alert_key, alert))
        
        # Resolve alerts that are no longer active
        for alert_key, alert in alerts_to_resolve:
            if alert.id:
                self._resolve_alert(alert.id)
            del self._active_alerts[alert_key]
            resolved_count += 1
            
            logger.info(
                f"Auto-resolved alert {alert.id} for {alert.component_type.value} "
                f"on vessel {alert.vessel_id} - condition no longer present"
            )
        
        status_summary = {
            'maintained_alerts': maintained_count,
            'resolved_alerts': resolved_count,
            'new_alerts': new_count,
            'total_active_alerts': len(self._active_alerts)
        }
        
        logger.info(
            f"Alert status maintenance complete: "
            f"{maintained_count} maintained, {resolved_count} resolved, "
            f"{new_count} new, {len(self._active_alerts)} total active"
        )
        
        return status_summary
    
    def get_comprehensive_alert_log(
        self,
        vessel_id: Optional[str] = None,
        days_back: int = 7
    ) -> List[Dict[str, Any]]:
        """
        Get comprehensive alert log for audit trails and historical tracking.
        
        Args:
            vessel_id: Optional filter by vessel ID
            days_back: Number of days of history to retrieve
            
        Returns:
            List of alert log entries with full audit information
        """
        # This would typically query the database for historical alert data
        # For now, return current active alerts with audit information
        
        alert_log = []
        
        for alert in self._active_alerts.values():
            if vessel_id and alert.vessel_id != vessel_id:
                continue
            
            log_entry = {
                'alert_id': alert.id,
                'vessel_id': alert.vessel_id,
                'component_type': alert.component_type.value,
                'alert_type': alert.alert_type.value,
                'severity': alert.severity.value,
                'message': alert.message,
                'created_at': alert.created_at.isoformat(),
                'is_resolved': alert.is_resolved,
                'resolved_at': alert.resolved_at.isoformat() if alert.resolved_at else None,
                'metadata': alert.metadata,
                'audit_trail': {
                    'detection_method': 'automated_monitoring',
                    'threshold_exceeded': True,
                    'requires_action': alert.metadata.get('requires_ticket_creation', False),
                    'ticket_status': 'created' if alert.metadata.get('ticket_created', False) else 'pending'
                }
            }
            
            alert_log.append(log_entry)
        
        # Sort by creation time (newest first)
        alert_log.sort(key=lambda x: x['created_at'], reverse=True)
        
        return alert_log
    
    def cleanup_resolved_alerts(self) -> int:
        """
        Clean up resolved alerts from memory.
        
        Returns:
            Number of alerts cleaned up
        """
        initial_count = len(self._active_alerts)
        
        # Remove alerts that have been resolved (this would typically check database)
        # For now, we'll keep all alerts in memory until explicitly resolved
        
        cleaned_count = initial_count - len(self._active_alerts)
        
        if cleaned_count > 0:
            logger.info(f"Cleaned up {cleaned_count} resolved alerts from memory")
        
        return cleaned_count